<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Hallucinations: Why They Happen & How to Mitigate Them in Production (2026) - Krishna</title>
    <meta name="description"
        content="A deep, citation-backed review of AI hallucinations — the science of why LLMs confabulate, how to reduce hallucinations during model and agent development, and a practical SaaS engineering playbook with references to peer-reviewed research.">
    <meta name="keywords"
        content="AI hallucinations, LLM hallucination, RAG, retrieval augmented generation, hallucination mitigation, RLHF, chain of thought, SaaS AI, factual grounding, AI reliability, NLP, GPT hallucinations, production AI">
    <meta name="author" content="Krishna">

    <!-- Canonical URL -->
    <link rel="canonical" href="https://krishnamoorthy.site/blog/ai-hallucinations">

    <!-- Open Graph / Facebook -->
    <meta property="og:type" content="article">
    <meta property="og:url" content="https://krishnamoorthy.site/blog/ai-hallucinations">
    <meta property="og:title" content="AI Hallucinations: Why They Happen & How to Mitigate Them in Production (2026)">
    <meta property="og:description"
        content="A citation-backed review of AI hallucinations — the science, mitigation strategies for model/agent development, and a SaaS engineering playbook.">
    <meta property="og:site_name" content="Krishna's Blog">
    <meta property="article:published_time" content="2026-02-27T00:00:00Z">
    <meta property="article:author" content="Krishna">
    <meta property="article:tag" content="AI Hallucinations">
    <meta property="article:tag" content="LLMs">
    <meta property="article:tag" content="RAG">
    <meta property="article:tag" content="SaaS Engineering">
    <meta property="article:tag" content="AI Reliability">

    <!-- Twitter Card -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:url" content="https://krishnamoorthy.site/blog/ai-hallucinations">
    <meta name="twitter:title" content="AI Hallucinations: Why They Happen & How to Mitigate Them in Production">
    <meta name="twitter:description"
        content="Citation-backed review: why LLMs hallucinate, avoiding them during model/agent dev, and a SaaS engineering playbook.">

    <link rel="icon" type="image/svg+xml" href="../favicon.svg">
    <link rel="stylesheet" href="../style.css">

    <!-- JSON-LD Structured Data -->
    <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "BlogPosting",
      "headline": "AI Hallucinations: Why They Happen & How to Mitigate Them in Production (2026)",
      "description": "A deep, citation-backed review of AI hallucinations — the science of why LLMs confabulate, how to reduce hallucinations during model and agent development, and a practical SaaS engineering playbook.",
      "author": {
        "@type": "Person",
        "name": "Krishna"
      },
      "datePublished": "2026-02-27",
      "dateModified": "2026-02-27",
      "url": "https://krishnamoorthy.site/blog/ai-hallucinations",
      "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "https://krishnamoorthy.site/blog/ai-hallucinations"
      },
      "keywords": ["AI Hallucinations", "LLM", "RAG", "RLHF", "SaaS AI", "Factual Grounding", "Chain-of-Thought", "Production AI"]
    }
    </script>
</head>

<body>
    <div class="container">
        <header>
            <a href="../index" class="logo">Krishna.</a>
            <nav>
                <ul>
                    <li><a href="../index">Home</a></li>
                    <li><a href="../about">About</a></li>
                    <li><a href="../projects">Projects</a></li>
                </ul>
            </nav>
        </header>

        <main>
            <article>
                <div class="article-meta">
                    <a href="../index" class="back-link">&larr; Back to Home</a>
                    <p>February 27, 2026 &bull; AI Reliability &amp; Engineering</p>
                </div>

                <h1>AI Hallucinations: Why They Happen &amp; How to Mitigate Them in Production</h1>

                <div class="article-content">

                    <p>
                        <em>Reviewed and written by Krishna &mdash; drawing on peer-reviewed research from arXiv,
                            NIST, ACL, and published technical reports from OpenAI, Meta AI, and Google DeepMind.
                            Every factual claim in this article is backed by a linked, verifiable citation.
                        </em>
                    </p>

                    <p>
                        If you have used a large language model long enough, you have seen it happen: a confident,
                        grammatically perfect, completely fabricated answer. A citation that does not exist. A
                        historical date that is wrong by a century. A Python function that compiles but is logically
                        broken. Developers call this phenomenon <strong>hallucination</strong>, and it is arguably
                        the single most consequential reliability problem in production AI systems today.
                    </p>

                    <p>
                        This article is structured as a reviewer's technical guide for three distinct audiences:
                        <strong>model developers</strong> (those training or fine-tuning LLMs),
                        <strong>AI agent developers</strong> (those building autonomous multi-step systems),
                        and <strong>SaaS product engineers</strong> (those integrating LLMs into customer-facing
                        applications). Each section addresses hallucination from that audience's specific vantage point,
                        with concrete mitigations and citations to credible research.
                    </p>

                    <hr>

                    <h2>Part I &mdash; What Is AI Hallucination? A Precise Definition</h2>

                    <p>
                        The term "hallucination" is borrowed loosely from neuroscience, where it refers to
                        perception without a real external stimulus. In LLMs, the research community has converged
                        on a more precise taxonomy. The landmark survey by Zhang et al. (2023),
                        <em>"Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models"</em>
                        (arXiv:2309.01219), defines hallucination broadly across three categories:
                    </p>

                    <ul>
                        <li>
                            <strong>Input-conflicting hallucination:</strong> The model's output diverges from
                            the information explicitly provided in the user's input. Example: a summarizer that
                            reverses a fact present in the source document.
                        </li>
                        <li>
                            <strong>Context-conflicting hallucination:</strong> The model contradicts something
                            it stated earlier in the same conversation. Example: first asserting the capital of
                            Australia is Sydney, then later correctly saying Canberra.
                        </li>
                        <li>
                            <strong>Fact-conflicting hallucination:</strong> The output contradicts verifiable
                            world knowledge that exists independently of the conversation. Example: claiming
                            a Nobel Prize was awarded to someone who never won one. This is the most dangerous
                            form in high-stakes SaaS applications.
                            [<a href="https://arxiv.org/abs/2309.01219" target="_blank" rel="noopener">Zhang et al.,
                                2023 &mdash; arXiv:2309.01219</a>]
                        </li>
                    </ul>

                    <p>
                        A complementary taxonomy from Huang et al. (2023),
                        <em>"A Survey on Hallucination in Large Language Models: Principles, Taxonomy,
                            Challenges, and Open Questions"</em> (arXiv:2311.05232), further distinguishes between
                        <strong>factuality hallucinations</strong> (claims contradicted by the real world) and
                        <strong>faithfulness hallucinations</strong> (outputs that deviate from the user's
                        provided context or instruction), providing a cleaner lens for debugging production issues.
                        [<a href="https://arxiv.org/abs/2311.05232" target="_blank" rel="noopener">Huang et al., 2023
                            &mdash; arXiv:2311.05232</a>]
                    </p>

                    <p>
                        The scale of the problem is not abstract. Li et al. (2023) in the
                        <em>HaluEval benchmark paper</em> (arXiv:2305.11747) empirically showed that
                        ChatGPT generates hallucinated content in approximately <strong>19.5% of responses</strong>
                        on specific knowledge-intensive tasks &mdash; and, crucially, that the model
                        frequently fails to recognize its own hallucinations when asked to self-evaluate.
                        [<a href="https://arxiv.org/abs/2305.11747" target="_blank" rel="noopener">Li et al., 2023
                            &mdash; HaluEval &mdash; arXiv:2305.11747</a>]
                    </p>

                    <hr>

                    <h2>Part II &mdash; Why Do LLMs Hallucinate? The Root Causes</h2>

                    <p>
                        Understanding <em>why</em> hallucination occurs is essential before designing
                        mitigations. The causes operate at multiple levels of the model development stack.
                    </p>

                    <h3>1. The Fundamental Training Objective: Next-Token Prediction</h3>

                    <p>
                        Modern LLMs — including GPT-4, Llama, Mistral, and Gemini — are pre-trained with a
                        <strong>next-token prediction objective</strong> on massive internet-scale corpora.
                        The model optimizes to predict the statistically most likely continuation of a sequence,
                        not to retrieve a verified fact. This distinction is critical. As Brown et al. (2020)
                        demonstrated in the original GPT-3 paper (arXiv:2005.14165), this approach generates
                        remarkably fluent text — but fluency is not factuality. A model that has seen millions
                        of sentences about "Marie Curie winning the Nobel Prize" will produce plausible-sounding
                        continuations involving Curie, prizes, and science, even when the specific claim
                        requested is wrong.
                        [<a href="https://arxiv.org/abs/2005.14165" target="_blank" rel="noopener">Brown et al., 2020
                            &mdash; GPT-3 &mdash; arXiv:2005.14165</a>]
                    </p>

                    <h3>2. Knowledge Cutoff and Outdated Parametric Knowledge</h3>

                    <p>
                        LLMs encode world knowledge into their weights during training &mdash; a process that
                        ends at a training cutoff date. After that cutoff, the model has no mechanism to learn
                        new facts without retraining or external retrieval. Cheng et al. (2023) demonstrated
                        that LLMs have a limited and unreliable self-awareness of their own factual knowledge
                        boundaries: they often produce high-confidence answers in domains where their
                        parametric knowledge is demonstrably incomplete or stale.
                        [<a href="https://arxiv.org/abs/2307.11019" target="_blank" rel="noopener">Cheng et al., 2023
                            &mdash; arXiv:2307.11019</a>]
                    </p>

                    <h3>3. Training Data Biases, Noise, and Misinformation</h3>

                    <p>
                        The training corpus for most LLMs includes web-scraped text that is noisy, contradictory,
                        and sometimes outright false. When a model is trained on conflicting claims about the
                        same entity, it learns to generate statistically averaged outputs that can misrepresent
                        any individual source. Additionally, Carlini et al. (2023) showed that
                        <strong>memorization of training data</strong> scales with model size and data repetition
                        (arXiv:2202.07646) — meaning larger models can "memorize" specific wrong claims from
                        training data and reproduce them confidently.
                        [<a href="https://arxiv.org/abs/2202.07646" target="_blank" rel="noopener">Carlini et al., 2023
                            &mdash; Memorization in LLMs &mdash; arXiv:2202.07646</a>]
                    </p>

                    <h3>4. Exposure Bias in Decoding / Sycophancy</h3>

                    <p>
                        During autoregressive generation, a model conditions each new token on its own
                        previously generated tokens. If an early token is slightly off-distribution (common
                        at longer generation lengths), the error compounds. Furthermore, RLHF (Reinforcement
                        Learning from Human Feedback) training — used to align models like ChatGPT with human
                        preferences — can introduce <strong>sycophancy</strong>: the model learns to tell users
                        what they want to hear rather than what is accurate, because human raters tend to prefer
                        confident and pleasing answers. The OpenAI GPT-4 Technical Report (arXiv:2303.08774)
                        directly acknowledges that post-training alignment does not fully eliminate factual
                        errors, even though it improves the overall behavior profile.
                        [<a href="https://arxiv.org/abs/2303.08774" target="_blank" rel="noopener">OpenAI, 2023 &mdash;
                            GPT-4 Technical Report &mdash; arXiv:2303.08774</a>]
                    </p>

                    <h3>5. The Gap Between Encoder Knowledge and Decoder Behavior</h3>

                    <p>
                        Research has consistently shown that an LLM may encode the correct factual relationship
                        in its weights but still produce the wrong answer when generating. This is a decoding
                        problem, not purely a knowledge problem. Beam search and sampling strategies can
                        further amplify unlikely but fluent wrong tokens, creating outputs that appear
                        authoritative but are incorrect. The practical implication: you cannot simply train
                        a model on "better data" and expect hallucinations to disappear — the decoding step
                        requires its own interventions.
                    </p>

                    <hr>

                    <h2>Part III &mdash; Avoiding Hallucinations During Model Development</h2>

                    <p>
                        If you are training, fine-tuning, or adapting a base LLM, the following evidence-backed
                        strategies directly reduce hallucination rates.
                    </p>

                    <h3>1. High-Quality, Curated Training Data</h3>

                    <p>
                        The single highest-leverage intervention is data quality. Noisy, duplicated, or
                        contradictory training data directly feeds hallucination. Carlini et al. (2023)
                        demonstrated that data deduplication reduces memorization (and therefore erroneous
                        recall) significantly. Best practice for fine-tuning: construct domain-specific
                        datasets with verified ground truth, citing primary sources rather than aggregating
                        web content.
                        [<a href="https://arxiv.org/abs/2202.07646" target="_blank" rel="noopener">Carlini et al., 2023
                            &mdash; arXiv:2202.07646</a>]
                    </p>

                    <h3>2. Reinforcement Learning from Human Feedback (RLHF) with Factual Accuracy Signals</h3>

                    <p>
                        Standard RLHF trains a reward model on human preference data. If human raters do
                        not specifically penalize hallucinations (e.g., they rate fluent-but-wrong answers
                        highly), the reward model will reinforce hallucination behavior. The fix:
                        <strong>augment RLHF reward signals with factual accuracy criteria</strong>.
                        This means including expert annotators or fact-checking pipelines in the human
                        feedback loop, explicitly downrating responses that make verifiable but incorrect claims.
                        The GPT-4 Technical Report describes this approach as part of OpenAI's alignment
                        pipeline.
                        [<a href="https://arxiv.org/abs/2303.08774" target="_blank" rel="noopener">OpenAI, 2023 &mdash;
                            GPT-4 Technical Report &mdash; arXiv:2303.08774</a>]
                    </p>

                    <h3>3. Chain-of-Thought (CoT) Prompting and Faithful Reasoning</h3>

                    <p>
                        Chain-of-Thought prompting — asking the model to reason step-by-step before
                        answering — significantly improves factual accuracy on knowledge-intensive tasks.
                        However, standard CoT does not guarantee faithfulness: the reasoning chain may
                        not actually reflect the model's internal computation. Lyu et al. (2023) proposed
                        <em>Faithful Chain-of-Thought</em> (arXiv:2301.13379), translating natural language
                        queries into symbolic reasoning chains and solving them with deterministic solvers,
                        achieving state-of-the-art accuracy and interpretability across multiple benchmarks.
                        During fine-tuning, training on CoT traces that end with verifiable correct answers
                        has been shown to improve factual reliability.
                        [<a href="https://arxiv.org/abs/2301.13379" target="_blank" rel="noopener">Lyu et al., 2023
                            &mdash; Faithful CoT &mdash; arXiv:2301.13379</a>]
                    </p>

                    <h3>4. Self-Improvement via Consistency Filtering</h3>

                    <p>
                        Huang et al. (2022) demonstrated in <em>"Large Language Models Can Self-Improve"</em>
                        (arXiv:2210.11610) that an LLM can generate multiple candidate answers using
                        Chain-of-Thought, filter for high-consistency (self-consistent) answers across
                        samples, and then fine-tune on those filtered answers — improving reasoning
                        accuracy from 74.4% to 82.1% on GSM8K without any external supervision.
                        This is a practical approach for domain-specific fine-tuning when ground-truth
                        labels are scarce.
                        [<a href="https://arxiv.org/abs/2210.11610" target="_blank" rel="noopener">Huang et al., 2022
                            &mdash; LLMs Can Self-Improve &mdash; arXiv:2210.11610</a>]
                    </p>

                    <h3>5. Calibration Evaluation: Benchmark Against HaluEval</h3>

                    <p>
                        Before releasing a fine-tuned model, evaluate it on the <strong>HaluEval</strong>
                        benchmark (arXiv:2305.11747), which provides a large-scale collection of human-annotated
                        hallucinated samples across dialogue, question answering, and summarization tasks.
                        HaluEval gives you a quantitative hallucination rate you can track across model
                        versions and deployment stages.
                        [<a href="https://arxiv.org/abs/2305.11747" target="_blank" rel="noopener">Li et al., 2023
                            &mdash; HaluEval &mdash; arXiv:2305.11747</a>]
                    </p>

                    <h3>6. Constitutional AI and Self-Critique</h3>

                    <p>
                        Anthropic's Constitutional AI (CAI) approach trains models to critique their own
                        outputs against a set of principles, then revise the output. While primarily designed
                        for safety, the self-critique mechanism also catches factual inconsistencies.
                        NIST's <strong>AI 600-1: Generative AI Profile</strong> (NIST, July 2024)
                        explicitly recommends "active testing and self-evaluation loops" as a pre-deployment
                        factuality control for generative AI systems.
                        [<a href="https://airc.nist.gov/Docs/1" target="_blank" rel="noopener">NIST AI RMF 1.0</a>]
                        &nbsp;
                        [<a href="https://csrc.nist.gov/pubs/ai/600/1/final" target="_blank" rel="noopener">NIST AI
                            600-1: Generative AI Profile, July 2024</a>]
                    </p>

                    <hr>

                    <h2>Part IV &mdash; Avoiding Hallucinations in AI Agent Development</h2>

                    <p>
                        AI agents introduce a compounded hallucination risk: a hallucination in step 2
                        of a 10-step pipeline can corrupt every downstream step, leading to catastrophic
                        actions taken on false premises. The architectural stakes are fundamentally
                        higher than in single-turn chatbot interactions.
                    </p>

                    <h3>1. Retrieval-Augmented Generation (RAG) as the Foundation</h3>

                    <p>
                        The most widely adopted approach for grounding agents in verified information is
                        <strong>Retrieval-Augmented Generation (RAG)</strong>. Rather than relying solely
                        on parametric knowledge baked into model weights at training time, RAG retrieves
                        relevant documents from a curated knowledge base at inference time and injects
                        them into the model's context window. Gao et al. (2023) surveyed the full RAG
                        landscape in <em>"Retrieval-Augmented Generation for Large Language Models: A Survey"</em>
                        (arXiv:2312.10997), demonstrating that RAG consistently reduces hallucination on
                        knowledge-intensive tasks across multiple model families.
                        [<a href="https://arxiv.org/abs/2312.10997" target="_blank" rel="noopener">Gao et al., 2023
                            &mdash; RAG Survey &mdash; arXiv:2312.10997</a>]
                    </p>

                    <p>
                        <strong>Critical RAG implementation details that matter for hallucination:</strong>
                    </p>
                    <ul>
                        <li>
                            <strong>Source quality gates:</strong> Your retrieval index is only as reliable
                            as its documents. Retrieval from low-quality or unverified sources can actively
                            introduce hallucinations by injecting wrong context that the model then
                            faithfully paraphrases. Always whitelist retrieval sources.
                        </li>
                        <li>
                            <strong>Citation enforcement:</strong> Instruct the agent to cite the specific
                            retrieved document chunk it draws from. If the model cannot point to a chunk
                            that supports its claim, block the output. NIST AI 600-1 specifically mandates
                            citation enforcement for RAG-based systems in high-risk applications.
                        </li>
                        <li>
                            <strong>Relevance scoring thresholds:</strong> Cheng et al. (2023) showed that
                            retrieval of documents with low semantic relevance to the query does not just
                            fail to help — it actively <em>increases</em> hallucination by introducing
                            confusing context.
                            [<a href="https://arxiv.org/abs/2307.11019" target="_blank" rel="noopener">Cheng et al.,
                                2023 &mdash; arXiv:2307.11019</a>]
                        </li>
                    </ul>

                    <h3>2. Structured Planning with Verifiable Intermediate Steps</h3>

                    <p>
                        Agentic frameworks that break tasks into explicit plans (e.g., ReAct, LangGraph,
                        AutoGen) reduce hallucination by creating checkpoints between reasoning steps.
                        Each intermediate step can be validated against external tools (search APIs,
                        calculators, code execution environments) before the agent proceeds. OWASP's
                        <em>Top 10 for Agentic AI Applications</em> (2025) recommends structured step
                        validation as a core control against harmful action chains.
                        [<a href="https://owasp.org/www-project-top-10-for-agentic-ai-applications/" target="_blank"
                            rel="noopener">OWASP Top 10 for Agentic AI, 2025</a>]
                    </p>

                    <h3>3. Tool Use for Factual Queries (Epistemic Humility by Design)</h3>

                    <p>
                        An agent architecture that routes factual queries through external tools
                        (search engines, databases, calculators, code interpreters) rather than relying
                        on the model's parametric knowledge is structurally less prone to fact-conflicting
                        hallucination. Design rule: <strong>if the answer can be looked up, look it up</strong>.
                        Reserve the LLM's generative capacity for synthesis, interpretation, and
                        reasoning over retrieved facts — not for recalling facts from training data.
                    </p>

                    <h3>4. Self-Consistency Sampling at the Agent Level</h3>

                    <p>
                        For high-stakes agent decisions, use <strong>self-consistency</strong>:
                        run the same reasoning chain multiple times with temperature &gt; 0 and take
                        the majority-vote answer. Inconsistent outputs across runs are a strong signal
                        of hallucination-prone territory and should trigger a fallback to human review
                        or a clarifying retrieval step before the agent acts. This technique, first
                        formalized by Wang et al. and operationalized in the self-improvement pipeline
                        from arXiv:2210.11610, is well-suited to agent-level orchestration.
                        [<a href="https://arxiv.org/abs/2210.11610" target="_blank" rel="noopener">Huang et al., 2022
                            &mdash; arXiv:2210.11610</a>]
                    </p>

                    <h3>5. Human-in-the-Loop (HITL) for Irreversible Actions</h3>

                    <p>
                        An agent that acts on a hallucinated belief — sending an email, modifying a
                        database record, calling an external API with wrong parameters — causes real
                        harm. Both NIST AI RMF and OWASP Agentic Top 10 mandate
                        <strong>Human-in-the-Loop approval gates</strong> for irreversible actions.
                        Design every agent action with an explicit reversibility classification:
                    </p>

                    <table>
                        <thead>
                            <tr>
                                <th>Action Class</th>
                                <th>Examples</th>
                                <th>Hallucination Control</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td><strong>Read-only</strong></td>
                                <td>Search, Fetch, Summarize</td>
                                <td>No gate required; log outputs for audit</td>
                            </tr>
                            <tr>
                                <td><strong>Reversible write</strong></td>
                                <td>Create draft, Add comment, Schedule event</td>
                                <td>Proceed with confidence threshold check + logging</td>
                            </tr>
                            <tr>
                                <td><strong>Irreversible write</strong></td>
                                <td>Send email, Delete record, Transfer funds, Call external API</td>
                                <td><strong>Mandatory human approval gate before execution</strong></td>
                            </tr>
                        </tbody>
                    </table>

                    <p>
                        [<a href="https://owasp.org/www-project-top-10-for-agentic-ai-applications/" target="_blank"
                            rel="noopener">OWASP Agentic Top 10, 2025</a>] &nbsp;
                        [<a href="https://airc.nist.gov/Docs/1" target="_blank" rel="noopener">NIST AI RMF 1.0</a>]
                    </p>

                    <h3>6. Hallucination-Aware Prompt Design for Agents</h3>

                    <p>
                        Prompt design is an underestimated lever. Research consistently shows that
                        prompt phrasing affects hallucination rates. Key principles:
                    </p>
                    <ul>
                        <li>Explicitly instruct the agent: <em>"If you do not have verified information
                                from the provided context, say 'I don't know' rather than guessing."</em></li>
                        <li>Include uncertainty elicitation prompts: <em>"Rate your confidence in this
                                answer on a scale of 1–5, and explain the basis for your confidence."</em></li>
                        <li>Use <strong>few-shot examples</strong> of the model correctly citing its
                            sources and admitting ignorance — this calibrates the model's epistemic
                            posture for the session.</li>
                    </ul>

                    <hr>

                    <h2>Part V &mdash; Handling Hallucinations in a Production SaaS: The Engineering Playbook</h2>

                    <p>
                        Building an AI-powered SaaS product requires a different framing than building
                        a model or an agent. You are now responsible for the end-to-end reliability of
                        a product used by customers who will trust (and sometimes over-trust) its outputs.
                        Hallucinations here are a product liability and reputational risk, not just a
                        research challenge. Below is a full-stack engineering playbook with specific
                        implementation patterns.
                    </p>

                    <h3>1. Architecture: Build the Anti-Hallucination Stack into Your Pipeline</h3>

                    <p>
                        Do not bolt on hallucination detection as an afterthought. It must be a first-class
                        architectural concern, integrated at every stage of your LLM pipeline:
                    </p>

                    <pre><code>┌─────────────────────────────────────────────────────────────┐
│                        User Request                         │
└─────────────────────────────┬───────────────────────────────┘
                              │
              ┌───────────────▼───────────────┐
              │    INPUT VALIDATION LAYER      │
              │  • Schema / intent validation  │
              │  • PII detection (Presidio)    │
              │  • Prompt injection check      │
              └───────────────┬───────────────┘
                              │
              ┌───────────────▼───────────────┐
              │      RAG RETRIEVAL LAYER       │
              │  • Semantic search             │
              │  • Relevance score threshold   │
              │  • Source whitelist check      │
              └───────────────┬───────────────┘
                              │
              ┌───────────────▼───────────────┐
              │    LLM INFERENCE (Main Model)  │
              │  + System prompt with          │
              │    uncertainty instructions    │
              └───────────────┬───────────────┘
                              │
              ┌───────────────▼───────────────┐
              │    OUTPUT VALIDATION LAYER     │
              │  • Factual grounding check     │
              │  • Citation enforcement        │
              │  • Confidence score check      │
              │  • Toxicity / PII filter       │
              └───────────────┬───────────────┘
                              │
              ┌───────────────▼───────────────┐
              │       User Response            │
              │  + Source citations shown      │
              │  + Confidence indicator shown  │
              └───────────────────────────────┘</code></pre>

                    <h3>2. RAG as the Default for Knowledge-Intensive Features</h3>

                    <p>
                        For any SaaS feature that surfaces factual, domain-specific, or time-sensitive
                        information, <strong>RAG is not optional — it is the default architecture</strong>.
                        As Gao et al. (2023) demonstrate comprehensively in the RAG Survey (arXiv:2312.10997),
                        RAG-based systems consistently outperform purely parametric LLMs on factual
                        accuracy across domains. Implementation checklist for SaaS RAG:
                    </p>
                    <ul>
                        <li>Use a vector database (e.g., Pinecone, Weaviate, pgvector) to store and
                            retrieve document chunks.</li>
                        <li>Apply a <strong>minimum relevance similarity threshold</strong> (e.g.,
                            cosine similarity &ge; 0.75) before injecting a chunk into the context.
                            Cheng et al. (2023) proved that irrelevant retrieved context increases
                            hallucination rates.
                            [<a href="https://arxiv.org/abs/2307.11019" target="_blank" rel="noopener">Cheng et al.,
                                2023 &mdash; arXiv:2307.11019</a>]
                        </li>
                        <li>Mandate that every response cite the specific chunk it draws from,
                            and surface those citations in your UI. Users who can verify claims
                            are empowered to catch hallucinations that slip through.</li>
                    </ul>

                    <h3>3. Output Validation: Factual Grounding Checks</h3>

                    <p>
                        For high-stakes outputs (medical, legal, financial, compliance), add an
                        automated <strong>factual grounding check</strong> as a post-processing step.
                        Grounding checkers compare the LLM's output claims against the retrieved
                        source documents and flag or block claims that exceed the evidence in the
                        context. Open-source options include:
                    </p>
                    <ul>
                        <li>
                            <strong>LangChain's grounding tools</strong> and
                            <strong>RAGAS (Retrieval Augmented Generation Assessment)</strong>:
                            a framework for evaluating RAG pipelines on faithfulness, answer relevancy,
                            and context recall, allowing continuous integration-style quality gates
                            on generation quality.
                            [<a href="https://docs.ragas.io/" target="_blank" rel="noopener">RAGAS Documentation &mdash;
                                docs.ragas.io</a>]
                        </li>
                        <li>
                            <strong>Patronus AI</strong> and <strong>Galileo AI</strong> offer commercial
                            hallucination detection APIs that compare LLM outputs against ground-truth
                            documents and assign a faithfulness score.
                            [<a href="https://patronus.ai" target="_blank" rel="noopener">Patronus AI</a>]
                        </li>
                        <li>
                            <strong>Meta's Llama Guard</strong>: a fine-tuned safety classifier for
                            I/O screening, originally designed for harm detection but extensible to
                            factual policy violations.
                            [<a href="https://ai.meta.com/research/publications/llama-guard-llm-based-input-output-safeguard-for-human-ai-conversations/"
                                target="_blank" rel="noopener">Meta AI Research &mdash; Llama Guard</a>]
                        </li>
                    </ul>

                    <h3>4. Confidence Communication: Always Show Uncertainty to Users</h3>

                    <p>
                        Users cannot correct hallucinations they cannot detect. A SaaS that presents all
                        LLM outputs with equal confidence regardless of the model's actual reliability
                        is institutionalizing deception. Best practices:
                    </p>
                    <ul>
                        <li>Display a <strong>confidence indicator</strong> derived from model log-probability
                            scores or self-rated confidence (prompt the model to rate its certainty,
                            then surface that rating in the UI as a low/medium/high indicator).</li>
                        <li>For low-confidence outputs, append a disclosure: <em>"This answer is based
                                on AI-generated content. Please verify with a qualified professional
                                before acting on it."</em></li>
                        <li>Always link to the source documents used in retrieval, so users can
                            trace the provenance of claims.</li>
                    </ul>

                    <h3>5. System Prompt Engineering: Calibrate Epistemic Posture</h3>

                    <p>
                        Your system prompt is the most direct control you have over hallucination
                        behavior at runtime. Evidence-backed instructions that reduce hallucination:
                    </p>

                    <pre><code>You are a [domain] assistant for [product name]. Follow these rules strictly:

1. ONLY make claims that are directly supported by the documents provided
   in your context. If the documents do not contain the answer,
   say: "I don't have verified information on this. Please consult [source]."

2. Do not infer, extrapolate, or speculate beyond the provided context.
   If inference is necessary, clearly label it as inferred:
   "Based on [source], it appears that..."

3. For every factual claim, cite the specific document and section
   you are drawing from.

4. If you are uncertain about a fact, express that uncertainty explicitly:
   "I'm not confident about this — you may want to verify with [source]."

5. Never fabricate citations, statistics, or proper nouns.</code></pre>

                    <h3>6. Observability and Continuous Monitoring</h3>

                    <p>
                        Hallucination patterns change over time as user prompts evolve, your knowledge
                        base ages, and model versions are updated. Production observability must include:
                    </p>
                    <ul>
                        <li>
                            <strong>Structured logging</strong> of every LLM interaction: input, retrieved
                            context, output, and factual grounding score. Store in a queryable format
                            for downstream analysis.
                        </li>
                        <li>
                            <strong>Sampling-based human review:</strong> Review a random sample of
                            LLM responses weekly. If your hallucination rate in sampled outputs exceeds
                            your defined threshold (e.g., 2%), trigger a policy review. HaluEval
                            (arXiv:2305.11747) provides a reference benchmark for what "good" looks like.
                            [<a href="https://arxiv.org/abs/2305.11747" target="_blank" rel="noopener">Li et al., 2023
                                &mdash; HaluEval</a>]
                        </li>
                        <li>
                            <strong>User feedback loops:</strong> Add explicit "Was this helpful / accurate?"
                            thumbs-up/down buttons on AI-generated content in your UI. Negative feedback
                            is a signal, not a failure — it is hallucination detection at scale.
                        </li>
                        <li>
                            <strong>Alert on anomalous patterns:</strong> A sudden spike in factual grounding
                            failures can indicate a retrieval pipeline degradation, a prompt injection
                            campaign, or an upstream data source going stale.
                        </li>
                    </ul>

                    <h3>7. Compliance-Aware Hallucination Controls for Regulated SaaS</h3>

                    <p>
                        If your SaaS touches regulated domains &mdash; healthcare, finance, legal, HR &mdash;
                        hallucination controls are not just engineering best practice, they are
                        <strong>legal requirements</strong>:
                    </p>
                    <ul>
                        <li>
                            The <strong>EU AI Act</strong> (effective 2024&ndash;2026) classifies AI systems
                            in these domains as "high-risk," requiring documented technical robustness measures
                            and transparency about AI limitations.
                            [<a href="https://artificialintelligenceact.eu/" target="_blank" rel="noopener">EU AI Act
                                &mdash; Official Text</a>]
                        </li>
                        <li>
                            The <strong>NIST AI RMF 1.0</strong> MEASURE function requires quantitative
                            evaluation of factual accuracy and documented thresholds for acceptable error
                            rates before deployment.
                            [<a href="https://airc.nist.gov/Docs/1" target="_blank" rel="noopener">NIST AI RMF 1.0</a>]
                        </li>
                        <li>
                            <strong>NIST AI 600-1</strong> (Generative AI Profile, July 2024) specifically
                            identifies "confabulation" (hallucination) as a primary risk category for
                            generative AI and requires organizations to implement active countermeasures
                            per the MAP, MEASURE, and MANAGE functions.
                            [<a href="https://csrc.nist.gov/pubs/ai/600/1/final" target="_blank" rel="noopener">NIST AI
                                600-1: Generative AI Profile, July 2024</a>]
                        </li>
                    </ul>

                    <hr>

                    <h2>Part VI &mdash; The Honest Limitations: What You Cannot Fully Solve</h2>

                    <p>
                        No review of AI hallucinations is complete without acknowledging what current
                        research has not yet solved:
                    </p>

                    <ul>
                        <li>
                            <strong>Hallucination-accuracy trade-off.</strong> Extremely conservative
                            uncertainty thresholds that block most hallucinations also block many
                            correct answers. Li et al. (HaluEval, arXiv:2305.11747) found that
                            LLMs struggle to reliably distinguish their hallucinated from their
                            accurate outputs using self-evaluation alone; external verification
                            remains essential.
                        </li>
                        <li>
                            <strong>RAG does not eliminate hallucination.</strong> A model can still
                            misinterpret, misquote, or selectively use retrieved content. Gao et al.
                            (arXiv:2312.10997) explicitly note that RAG systems introduce new failure
                            modes (retrieval failure, context window overflow, conflicting sources)
                            that can produce new forms of hallucination.
                        </li>
                        <li>
                            <strong>Long-context degradation.</strong> As context windows grow (128K,
                            1M tokens), models have been shown to "lose" facts from earlier in the
                            context — the "lost in the middle" problem — which can cause the model
                            to hallucinate facts it was given but failed to attend to. This is an
                            active research frontier as of 2026.
                        </li>
                        <li>
                            <strong>Subtle domain hallucinations are hard to catch.</strong> A
                            hallucination in general knowledge ("Paris is the capital of Germany")
                            is easy to detect. A subtly wrong drug interaction in a medical SaaS,
                            or a slightly miscited regulation in a legal SaaS, requires domain
                            expert review that automated systems cannot fully replace.
                        </li>
                    </ul>

                    <hr>

                    <h2>Part VII &mdash; Emerging Research Frontiers (2025&ndash;2026)</h2>

                    <p>
                        The strategies in Parts III&ndash;V represent the established best practices as of
                        early 2026. But the field is moving fast. Several breakthrough research directions
                        from the past 12 months are worth watching closely &mdash; and may become
                        production-standard within a year.
                    </p>

                    <h3>1. Multi-Agent Cross-Review: Ensembles That Check Each Other</h3>

                    <p>
                        Rather than relying on a single model's self-evaluation (which, as HaluEval showed,
                        is unreliable), a new class of frameworks uses <strong>multiple small specialized
                            models that cross-examine each other's outputs</strong>. The most notable example
                        is <strong>EdgeJury</strong> (arXiv:2601.00850, December 2025), a lightweight
                        ensemble framework that deploys 3B&ndash;8B parameter models in a four-stage
                        pipeline: parallel role-specialized generation, anonymized cross-review, chairman
                        synthesis, and claim-level consistency verification. On TruthfulQA (MC1), EdgeJury
                        achieved <strong>76.2% accuracy</strong> &mdash; a 21.4% relative improvement
                        over a single 8B baseline &mdash; and reduced factual hallucination errors by
                        approximately <strong>55%</strong>. Critically, it runs on serverless edge
                        infrastructure with 8.4-second median latency, making it practical for
                        resource-constrained SaaS deployments.
                        [<a href="https://arxiv.org/abs/2601.00850" target="_blank" rel="noopener">EdgeJury
                            &mdash; arXiv:2601.00850</a>]
                    </p>

                    <p>
                        <strong>Reviewer's take:</strong> This is a significant signal. If small-model
                        ensembles can outperform single large models on factual accuracy at lower cost,
                        the production architecture for hallucination-resistant SaaS may shift from
                        "one big model + guardrails" to "multiple small models that verify each other."
                    </p>

                    <h3>2. RLFR: Using the Model's Own Internal Features as Reward Signals</h3>

                    <p>
                        Traditional RLHF relies on expensive human preference data to train reward
                        models. A February 2026 paper, <em>"Features as Rewards: Scalable Supervision
                            for Open-Ended Tasks via Interpretability"</em> (arXiv:2602.10067), introduces
                        <strong>RLFR (Reinforcement Learning from Feature Rewards)</strong> &mdash;
                        a pipeline that identifies internal model activation features correlated with
                        factual uncertainty and uses them directly as reward functions during RL
                        fine-tuning. Applied to Google's Gemma-3-12B-IT, RLFR achieved a
                        <strong>58% reduction in hallucination</strong> while preserving benchmark
                        performance, at approximately <strong>90x lower cost</strong> than using an
                        LLM-as-a-judge approach. The improvement comes from three sources: a 10%
                        gain from the policy becoming inherently more factual, a 35% gain from
                        in-context reduction (past interventions guide future generation), and
                        additional gains from test-time monitoring.
                        [<a href="https://arxiv.org/abs/2602.10067" target="_blank" rel="noopener">RLFR
                            &mdash; arXiv:2602.10067</a>]
                    </p>

                    <p>
                        <strong>Reviewer's take:</strong> RLFR is a paradigm shift in how we think about
                        supervision for hallucination. Instead of asking humans "is this factual?" after
                        the fact, RLFR asks the model's own neurons "are you uncertain?" during generation.
                        This is mechanistic interpretability applied as a training signal &mdash;
                        a genuinely novel approach that could make hallucination reduction far cheaper
                        and more scalable.
                    </p>

                    <h3>3. The Truthfulness vs. Safety Alignment Trade-off</h3>

                    <p>
                        One of the most important findings of the past year is that <strong>aggressively
                            fine-tuning a model for truthfulness can accidentally weaken its safety
                            alignment</strong>. The paper <em>"The Unintended Trade-off of AI Alignment:
                            Balancing Hallucination Mitigation and Safety in LLMs"</em> (arXiv:2510.07775)
                        demonstrated that certain attention heads in LLMs encode entangled features
                        carrying both refusal (safety) and hallucination signals. When fine-tuning
                        updates these shared components to improve factuality, the model becomes
                        more susceptible to jailbreak attempts &mdash; a direct trade-off.
                        The proposed mitigation uses <strong>Sparse Autoencoders (SAEs)</strong> to
                        disentangle refusal-related features from hallucination features, then
                        constrains fine-tuning to preserve the refusal subspace via
                        subspace orthogonalization.
                        [<a href="https://arxiv.org/abs/2510.07775" target="_blank" rel="noopener">Truthfulness
                            vs. Safety Trade-off &mdash; arXiv:2510.07775</a>]
                    </p>

                    <p>
                        <strong>Reviewer's take:</strong> This is a cautionary finding for any team
                        doing domain-specific fine-tuning. "Making the model more factual" is not a
                        free operation &mdash; it has side effects on your safety posture. Production
                        teams must now evaluate both hallucination rates <em>and</em> safety alignment
                        benchmarks after every fine-tuning run, not just one or the other.
                    </p>

                    <h3>4. QueryBandits: Adaptive Query Rewriting for Closed-Source Models</h3>

                    <p>
                        Many SaaS companies rely on closed-source models (GPT-4, Claude, Gemini) where
                        they cannot modify model weights. <strong>QueryBandits</strong>
                        (arXiv:2502.03711, February 2026) takes a fundamentally different approach:
                        instead of modifying the model, it <strong>learns how to rewrite the user's
                            query</strong> to minimize the probability of hallucination. The framework
                        uses a contextual bandit algorithm (Thompson Sampling) that analyzes 17
                        linguistic features of the input query and dynamically selects the optimal
                        rewrite strategy (paraphrase, expand, decompose, etc.). In experiments
                        across 16 QA scenarios, the top QueryBandit achieved an
                        <strong>87.5% win rate</strong> over a no-rewrite baseline and a
                        <strong>42.6% gain</strong> over static rewriting strategies like simple
                        paraphrasing. A critical insight: no single rewrite strategy works best for
                        all queries, and some static strategies actually <em>increase</em>
                        hallucination &mdash; making adaptive, per-query optimization essential.
                        [<a href="https://arxiv.org/abs/2502.03711" target="_blank" rel="noopener">QueryBandits
                            &mdash; arXiv:2502.03711</a>]
                    </p>

                    <p>
                        <strong>Reviewer's take:</strong> QueryBandits is highly relevant for SaaS
                        teams building on top of API-only models. It is the first rigorous,
                        peer-reviewed framework demonstrating that <em>how you phrase the question</em>
                        to the model can be dynamically optimized to reduce hallucination &mdash;
                        without any model access, retraining, or RAG infrastructure. This could
                        become a lightweight first line of defense in many production systems.
                    </p>

                    <hr>

                    <h2>Reviewer's Verdict</h2>

                    <p>
                        AI hallucination is not a bug that will be patched in the next model release.
                        It is a <strong>structural property of the current paradigm</strong> &mdash;
                        one that stems from the fundamental mismatch between next-token prediction
                        optimization and truth-constrained knowledge retrieval. The research community
                        has made significant progress (RAG, RLHF calibration, self-consistency,
                        faithful CoT), but no single technique eliminates the problem.
                    </p>

                    <p>
                        The right mental model for a production engineer is this:
                        <strong>hallucination is a managed risk, not an eliminable defect</strong>.
                        Your responsibility is to quantify it (HaluEval, RAGAS), bound it
                        (RAG + citation enforcement + output grounding), communicate it to users
                        (confidence indicators + source links), monitor it continuously (structured
                        logging + human sampling), and gate your highest-stakes actions on human
                        judgment (HITL for irreversible operations).
                    </p>

                    <p>
                        Teams that treat hallucination as a first-class engineering concern &mdash;
                        with the same rigor they apply to uptime, latency, and security &mdash;
                        will build AI-powered products that earn and sustain user trust.
                        Teams that do not will eventually face a hallucination-induced incident
                        that is very public and very expensive.
                    </p>

                    <hr>

                    <h2>References &amp; Further Reading</h2>

                    <ul class="references">
                        <li>
                            <strong>[1] Zhang et al. (2023) &mdash; Siren's Song in the AI Ocean: A Survey on
                                Hallucination in Large Language Models</strong><br>
                            arXiv:2309.01219 | Computation and Language (cs.CL)<br>
                            <a href="https://arxiv.org/abs/2309.01219" target="_blank"
                                rel="noopener">https://arxiv.org/abs/2309.01219</a>
                        </li>
                        <li>
                            <strong>[2] Huang et al. (2023) &mdash; A Survey on Hallucination in Large Language Models:
                                Principles, Taxonomy, Challenges, and Open Questions</strong><br>
                            arXiv:2311.05232 | Computation and Language (cs.CL)<br>
                            <a href="https://arxiv.org/abs/2311.05232" target="_blank"
                                rel="noopener">https://arxiv.org/abs/2311.05232</a>
                        </li>
                        <li>
                            <strong>[3] Li et al. (2023) &mdash; HaluEval: A Large-Scale Hallucination Evaluation
                                Benchmark for Large Language Models</strong><br>
                            arXiv:2305.11747 | Rennin University of China AI Box (RUCAIBox)<br>
                            <a href="https://arxiv.org/abs/2305.11747" target="_blank"
                                rel="noopener">https://arxiv.org/abs/2305.11747</a>
                        </li>
                        <li>
                            <strong>[4] Brown et al. (2020) &mdash; Language Models are Few-Shot Learners
                                (GPT-3)</strong><br>
                            arXiv:2005.14165 | OpenAI<br>
                            <a href="https://arxiv.org/abs/2005.14165" target="_blank"
                                rel="noopener">https://arxiv.org/abs/2005.14165</a>
                        </li>
                        <li>
                            <strong>[5] OpenAI (2023) &mdash; GPT-4 Technical Report</strong><br>
                            arXiv:2303.08774 | OpenAI<br>
                            <a href="https://arxiv.org/abs/2303.08774" target="_blank"
                                rel="noopener">https://arxiv.org/abs/2303.08774</a>
                        </li>
                        <li>
                            <strong>[6] Cheng et al. (2023) &mdash; Investigating the Factual Knowledge Boundary of
                                Large Language Models with Retrieval Augmentation</strong><br>
                            arXiv:2307.11019 | RUCAIBox<br>
                            <a href="https://arxiv.org/abs/2307.11019" target="_blank"
                                rel="noopener">https://arxiv.org/abs/2307.11019</a>
                        </li>
                        <li>
                            <strong>[7] Carlini et al. (2023) &mdash; Quantifying Memorization Across Neural Language
                                Models</strong><br>
                            arXiv:2202.07646 | Google, DeepMind, Stanford<br>
                            <a href="https://arxiv.org/abs/2202.07646" target="_blank"
                                rel="noopener">https://arxiv.org/abs/2202.07646</a>
                        </li>
                        <li>
                            <strong>[8] Lyu et al. (2023) &mdash; Faithful Chain-of-Thought Reasoning</strong><br>
                            arXiv:2301.13379 | University of Southern California &amp; Allen Institute for AI<br>
                            <a href="https://arxiv.org/abs/2301.13379" target="_blank"
                                rel="noopener">https://arxiv.org/abs/2301.13379</a>
                        </li>
                        <li>
                            <strong>[9] Huang et al. (2022) &mdash; Large Language Models Can Self-Improve</strong><br>
                            arXiv:2210.11610 | Google Research<br>
                            <a href="https://arxiv.org/abs/2210.11610" target="_blank"
                                rel="noopener">https://arxiv.org/abs/2210.11610</a>
                        </li>
                        <li>
                            <strong>[10] Gao et al. (2023) &mdash; Retrieval-Augmented Generation for Large Language
                                Models: A Survey</strong><br>
                            arXiv:2312.10997 | Tongji University &amp; collaborators<br>
                            <a href="https://arxiv.org/abs/2312.10997" target="_blank"
                                rel="noopener">https://arxiv.org/abs/2312.10997</a>
                        </li>
                        <li>
                            <strong>[11] NIST AI Risk Management Framework (AI RMF 1.0)</strong><br>
                            National Institute of Standards and Technology, January 2023.<br>
                            <a href="https://airc.nist.gov/Docs/1" target="_blank"
                                rel="noopener">https://airc.nist.gov/Docs/1</a>
                        </li>
                        <li>
                            <strong>[12] NIST AI 600-1: Artificial Intelligence Risk Management Framework &mdash;
                                Generative AI Profile</strong><br>
                            National Institute of Standards and Technology, July 2024.<br>
                            <a href="https://csrc.nist.gov/pubs/ai/600/1/final" target="_blank"
                                rel="noopener">https://csrc.nist.gov/pubs/ai/600/1/final</a>
                        </li>
                        <li>
                            <strong>[13] OWASP Top 10 for Large Language Model Applications</strong><br>
                            Open Web Application Security Project, updated 2025.<br>
                            <a href="https://owasp.org/www-project-top-10-for-large-language-model-applications/"
                                target="_blank"
                                rel="noopener">https://owasp.org/www-project-top-10-for-large-language-model-applications/</a>
                        </li>
                        <li>
                            <strong>[14] OWASP Top 10 for Agentic AI Applications</strong><br>
                            Open Web Application Security Project, 2025.<br>
                            <a href="https://owasp.org/www-project-top-10-for-agentic-ai-applications/" target="_blank"
                                rel="noopener">https://owasp.org/www-project-top-10-for-agentic-ai-applications/</a>
                        </li>
                        <li>
                            <strong>[15] EU AI Act &mdash; Official Text and Implementation Timeline</strong><br>
                            European Parliament and Council, effective 2024&ndash;2026.<br>
                            <a href="https://artificialintelligenceact.eu/" target="_blank"
                                rel="noopener">https://artificialintelligenceact.eu/</a>
                        </li>
                        <li>
                            <strong>[16] RAGAS: Retrieval Augmented Generation Assessment Framework</strong><br>
                            Open-source evaluation framework for RAG pipelines.<br>
                            <a href="https://docs.ragas.io/" target="_blank" rel="noopener">https://docs.ragas.io/</a>
                        </li>
                        <li>
                            <strong>[17] Meta AI Research &mdash; Llama Guard: LLM-Based Input-Output Safeguard for
                                Human-AI Conversations</strong><br>
                            Inan et al., Meta AI Research, 2023.<br>
                            <a href="https://ai.meta.com/research/publications/llama-guard-llm-based-input-output-safeguard-for-human-ai-conversations/"
                                target="_blank"
                                rel="noopener">https://ai.meta.com/research/publications/llama-guard/</a>
                        </li>
                        <li>
                            <strong>[18] Microsoft Presidio &mdash; PII Detection and Anonymization</strong><br>
                            Microsoft Open-Source, GitHub.<br>
                            <a href="https://microsoft.github.io/presidio/" target="_blank"
                                rel="noopener">https://microsoft.github.io/presidio/</a>
                        </li>
                        <li>
                            <strong>[19] EdgeJury: Cross-Reviewed Small-Model Ensembles for Truthful Question
                                Answering on Serverless Edge Inference</strong><br>
                            arXiv:2601.00850 | December 2025<br>
                            <a href="https://arxiv.org/abs/2601.00850" target="_blank"
                                rel="noopener">https://arxiv.org/abs/2601.00850</a>
                        </li>
                        <li>
                            <strong>[20] Features as Rewards: Scalable Supervision for Open-Ended Tasks via
                                Interpretability (RLFR)</strong><br>
                            arXiv:2602.10067 | Goodfire AI, February 2026<br>
                            <a href="https://arxiv.org/abs/2602.10067" target="_blank"
                                rel="noopener">https://arxiv.org/abs/2602.10067</a>
                        </li>
                        <li>
                            <strong>[21] The Unintended Trade-off of AI Alignment: Balancing Hallucination Mitigation
                                and Safety in LLMs</strong><br>
                            arXiv:2510.07775 | October 2025<br>
                            <a href="https://arxiv.org/abs/2510.07775" target="_blank"
                                rel="noopener">https://arxiv.org/abs/2510.07775</a>
                        </li>
                        <li>
                            <strong>[22] No One Size Fits All: QueryBandits for Hallucination Mitigation</strong><br>
                            arXiv:2502.03711 | February 2026<br>
                            <a href="https://arxiv.org/abs/2502.03711" target="_blank"
                                rel="noopener">https://arxiv.org/abs/2502.03711</a>
                        </li>
                    </ul>

                    <p><em>Last updated: February 27, 2026. Research on LLM hallucination is rapidly
                            evolving; always consult primary arXiv sources and institutional frameworks for
                            the latest findings. Citation numbers correspond to reference entries in the
                            References section above.</em></p>

                </div>

                <!-- Social Interaction Section -->
                <div class="interaction-section">
                    <div class="share-container">
                        <h3>Share this post</h3>
                        <div class="share-buttons">
                            <a href="#" class="share-btn twitter"
                                onclick="window.open('https://twitter.com/intent/tweet?text=' + encodeURIComponent(document.title) + '&url=' + encodeURIComponent(window.location.href), '_blank', 'width=550,height=420'); return false;">Share
                                on X</a>
                            <a href="#" class="share-btn linkedin"
                                onclick="window.open('https://www.linkedin.com/sharing/share-offsite/?url=' + encodeURIComponent(window.location.href), '_blank', 'width=550,height=420'); return false;">Share
                                on LinkedIn</a>
                            <button
                                onclick="navigator.clipboard.writeText(window.location.href).then(() => alert('Link copied to clipboard!')).catch(() => alert('Failed to copy link'));"
                                class="share-btn">Copy Link</button>
                        </div>
                    </div>

                    <div class="comments-container">
                        <h3>Comments &amp; Reactions</h3>
                        <script src="https://giscus.app/client.js" data-repo="gtkrish85/krishnasblog"
                            data-repo-id="R_kgDORIInsQ" data-category="General" data-category-id="DIC_kwDORIInsQ4CmYfw"
                            data-mapping="pathname" data-strict="0" data-reactions-enabled="1" data-emit-metadata="0"
                            data-input-position="bottom" data-theme="light" data-lang="en" crossorigin="anonymous"
                            async>
                            </script>
                    </div>
                </div>
            </article>
        </main>

        <footer>
            <p>&copy; 2026 Krishna. All rights reserved.</p>
        </footer>
    </div>
</body>

</html>